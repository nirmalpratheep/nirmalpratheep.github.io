<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Nirmal Pratheep Natarajan - Design Automation Software Engineer at AMD">
    <title>Nirmal Pratheep Natarajan</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <div class="page-wrapper">
        <div class="container">
            <!-- Sidebar -->
            <aside class="sidebar">
                <div class="profile">
                    <div class="avatar">NP</div>
                    <h1 class="name">Nirmal Pratheep Natarajan</h1>
                    <p class="tagline">Design Automation Software Engineer @ AMD</p>
                </div>
                <nav class="nav-links">
                    <a href="#highlights">Highlights</a>
                    <a href="#publications">Research & Publications</a>
                    <a href="#work-projects">Industry Experience</a>
                    <a href="#open-source">AI/ML Projects</a>
                    <a href="#skills">Skills</a>
                    <a href="#education">Education</a>
                    <a href="#certifications">Certifications</a>
                </nav>
                <nav class="social-links">
                    <a href="mailto:nirmalpratheep@gmail.com">‚úâ Email</a>
                    <a href="https://github.com/nirmalpratheep" target="_blank">‚åò GitHub</a>
                    <a href="https://linkedin.com/in/nirmalpratheep" target="_blank">in LinkedIn</a>
                </nav>
            </aside>

            <!-- Main Content -->
            <main class="main-content">
                <section class="section">
                    <p class="bio">
                        <strong>13+ years</strong> in software engineering at AMD & Xilinx, specializing in
                        <strong>AI/ML-driven
                            solutions</strong>
                        from research to production. Currently focused on <strong>LLM training pipelines</strong> (SFT,
                        GRPO, RLHF),
                        <strong>Agentic AI systems</strong>, and <strong>distributed training infrastructure</strong>.
                        Passionate about <strong>AI Alignment</strong>, <strong>Deep Reinforcement Learning</strong>,
                        and scaling <strong>large-scale model training</strong>.
                    </p>
                </section>

                <section class="section" id="highlights">
                    <h2>üî• Highlights</h2>
                    <ul class="compact-list">
                        <li><span class="label"><strong>LLM Pre-training</strong></span><span>Optimized <strong>1B
                                    parameter model</strong> (DeepSeek Gated Sparse Attention + YaRN); achieved
                                <strong>33% throughput improvement</strong> via custom Triton kernels, fused Flash
                                Attention</span></li>
                        <li><span class="label"><strong>LLM Post-training</strong></span><span>Built SFT ‚Üí GRPO pipeline
                                for <strong>Qwen 2.5 Math 1.5B</strong>; achieved <strong>14.2√ó zero-shot
                                    accuracy</strong> (2% ‚Üí 40.46%)</span></li>
                        <li><span class="label"><strong>ML/CV</strong></span><span>Trained <strong>ResNet-50</strong> to
                                <strong>77.4% Top-1</strong> on ImageNet-1K; Pre-trained <strong>135M LLM</strong> with
                                40k tokens/sec</span></li>
                        <li><span class="label"><strong>AI in EDA</strong></span><span>Deep RL for FloorPlan
                                optimization, LLM-based triage decisions, GNN feature extraction</span></li>
                        <li><span class="label"><strong>Distributed Systems</strong></span><span>Ray, LSF Farm,
                                Client/Server concurrent capture, <strong>3√ó throughput</strong></span></li>
                        <li><span class="label"><strong>Leadership</strong></span><span>Technical Lead of <strong>10+
                                    engineers</strong> across global sites</span></li>
                    </ul>
                </section>

                <section class="section" id="publications">
                    <h2>üìö Research & Publications</h2>
                    <ul class="compact-list">
                        <li><span class="label"><strong>Deep RL FloorPlan</strong></span><span>GTAC'25 & SPS Tech
                                Conference <em>(Finalist, arXiv pending)</em></span></li>
                        <li><span class="label"><strong>ML Delay Prediction</strong></span><span>GTAC'22 AMD Tech
                                Conference <em>(Finalist)</em></span></li>
                        <li><span class="label"><strong>Adaptive OFDM Pilots</strong></span><span>IEEE WAMICON
                                2009</span></li>
                    </ul>
                </section>

                <section class="section" id="work-projects">
                    <h2>üíº Industry Experience</h2>
                    <div class="project-list">
                        <div class="project">
                            <strong>Deep RL for Directive Optimization</strong>
                            <ul>
                                <li>Researched and developed <strong>Environment/RL models</strong> to solve directive
                                    optimization for EDA tool performance</li>
                                <li>Implemented <strong>GIN (Graph Isomorphism Network)</strong> for feature extraction
                                    from 1 to 15 million node graph netlists</li>
                                <li>Developed <strong>token-based feature collection</strong> to avoid normalization
                                    problems between train/test splits and eliminate feature variation distribution
                                    issues</li>
                                <li>Employed <strong>Grid, ASHA, and PBT</strong> hyperparameter search for automated
                                    optimization</li>
                                <li>Built <strong>Ray-based distributed training</strong> infrastructure for scalable RL
                                    model training across multiple nodes</li>
                                <li>Achieved 2% improvement in Placement quality while replacing manual tuning</li>
                            </ul>
                        </div>

                        <div class="project">
                            <strong>Tool Chain Optimization</strong>
                            <ul>
                                <li>Technical Team Lead of 10+ members; built tool profiling, linters, debugging tools,
                                    and dashboards</li>
                                <li>Automatic YAML semantic verifier auto code generation tool to validate syntax and
                                    contents for HW/SW assumptions</li>
                                <li>Developed tool that splits entire device using divide-and-conquer approach, parallel
                                    processing each division of the chip through <strong>LSF Farm</strong> and merging
                                    results to handle/manage both compute/memory and overall throughput; enabled
                                    handling of <strong>20x larger chip versions</strong> compared to initial chip size
                                </li>
                            </ul>
                        </div>
                        <div class="project">
                            <strong>Simulation Delay Capture Tools for 10nm/7nm/2nm FPGAs</strong>
                            <ul>
                                <li>Senior Team Member in design/development from scratch - mentor and guide 10+
                                    engineers</li>
                                <li>Designed Client/Server model using <strong>Boost Asio</strong> and <strong>Google
                                        Protobuf</strong>; enabled <strong>concurrent multi-capture</strong> with 3√ó
                                    throughput improvement</li>
                                <li>Built high-performance pattern recognition processing 3.5B datapoints via clustering
                                    and distributed computation</li>
                                <li>Developed graph compression techniques (1 Billion Instance Paths to 500K patterns)
                                    with Python analytics and visualization tools</li>
                            </ul>
                        </div>
                        <div class="project">
                            <strong>AI/ML Projects</strong>
                            <ul>
                                <li>Developing <strong>Agentic AI framework</strong> with <strong>LLM-driven decision
                                        making</strong> for autonomous triage and reruns</li>
                                <li>Developed <strong>delay prediction algorithms</strong> using <strong>ML
                                        techniques</strong></li>
                                <li>Built <strong>feature engineering pipeline</strong> using <strong>GNN
                                        models</strong> and graph clustering for EDA router tool Design complexity
                                    prediction</li>
                                <li>Part of the team developing <strong>regression framework</strong> with <strong>model
                                        monitoring</strong> and <strong>automated fine-tuning</strong> when performance
                                    degrades; handles both <strong>model drift</strong> and <strong>data drift</strong>
                                </li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section class="section" id="open-source">
                    <h2>üöÄ AI/ML Projects</h2>

                    <h3 class="category">RL Projects</h3>
                    <div class="project-list">
                        <div class="project">
                            <strong><a href="https://github.com/nirmalpratheep/Alignment_and_Reasoning_RL"
                                    target="_blank">Alignment_and_Reasoning_RL</a></strong>
                            <ul>
                                <li>Built complete <strong>LLM alignment pipeline</strong> for <strong>Qwen 2.5 Math
                                        1.5B</strong> on <strong>Math Dataset (Hendrycks 2021)</strong>: Baseline ‚Üí
                                    <strong>SFT</strong> ‚Üí <strong>GRPO RL</strong> achieving <strong>14.2√ó zero-shot
                                        accuracy</strong> (2.84% ‚Üí 40.46%)
                                </li>
                                <li>Implemented <strong>TRL GRPOTrainer</strong> with <strong>vLLM colocate
                                        mode</strong> for high-throughput RL rollouts; achieved <strong>96.72% format
                                        accuracy</strong></li>
                                <li>Designed <strong>Optuna + ASHA</strong> hyperparameter optimization with
                                    <strong>dual-GPU pipeline</strong> (training on GPU 0, vLLM eval on GPU 1)
                                </li>
                                <li><strong>W&B Experiments</strong>: <a
                                        href="https://wandb.ai/nirmalpratheep-self/math-sft-optuna-asha"
                                        target="_blank">HPO</a> | <a
                                        href="https://wandb.ai/nirmalpratheep-self/math-sft" target="_blank">SFT</a> |
                                    <a href="https://wandb.ai/nirmalpratheep-self/math-grpo-trl"
                                        target="_blank">GRPO</a>
                                </li>
                            </ul>
                        </div>
                        <div class="project">
                            <strong><a href="https://github.com/nirmalpratheep/RL-CarNavigationAgent"
                                    target="_blank">RL-CarNavigationAgent</a></strong>
                            <ul>
                                <li>Tuned <strong>RL hyperparameters</strong> (Gamma 0.95, Tau 0.005) using
                                    <strong>TensorBoard</strong>; achieved <strong>stable learning (< 1.0 KL
                                            Divergence)</strong>
                                </li>
                                <li>Optimized <strong>DQN agent</strong> physics parameters; visualized <strong>Average
                                        Reward</strong> trends and gradient norms</li>
                            </ul>
                        </div>
                    </div>

                    <h3 class="category">LLM Projects</h3>
                    <div class="project-list">
                        <div class="project">
                            <strong>1B Seedmodel Training Optimization</strong>
                            <ul>
                                <li>Optimized <strong>1B parameter model</strong> with <strong>DeepSeek-based Gated
                                        Sparse Attention</strong> and <strong>YaRN Embedding</strong></li>
                                <li>Performed comprehensive training infrastructure optimization: evaluated <strong>ZeRO
                                        configs</strong>, <strong>FSDP vs DeepSpeed</strong> strategies, and
                                    <strong>cost vs time tradeoffs</strong>
                                </li>
                                <li>Achieved <strong>33% throughput improvement</strong> through kernel-level
                                    optimizations: fused <strong>Flash Attention</strong>, custom <strong>Triton
                                        kernels</strong> for layer norm, rotary embedding, and sparse attention</li>
                                <li>Reduced GPU stalls and CUDA sync overhead; minimized memory footprint and kernel
                                    call count</li>
                                <li>Utilized <strong>Nsight Systems</strong>, <strong>Nsight Compute</strong>, and
                                    <strong>PyTorch Profiler</strong> for bottleneck analysis and performance tuning
                                </li>
                            </ul>
                        </div>
                        <div class="project">
                            <strong><a href="https://github.com/nirmalpratheep/SmolLMv2-PreTrain"
                                    target="_blank">SmolLMv2-PreTrain</a></strong>
                            <ul>
                                <li>Implemented <strong>SmolLM v2 pre-training</strong> (<strong>135M
                                        parameters</strong>) on <strong>FineWeb-Edu dataset</strong> with <strong>Flash
                                        Attention</strong></li>
                                <li>Achieved <strong>~40k tokens/sec throughput</strong> using <strong>Mixed Precision
                                        (BF16)</strong>; reduced loss from 11.6 to 0.0015</li>
                            </ul>
                        </div>
                        <div class="project">
                            <strong><a href="https://github.com/nirmalpratheep/MiniTamilBPETokenizer"
                                    target="_blank">MiniTamilBPETokenizer</a></strong>
                            <ul>
                                <li>Developed <strong>Tamil BPE tokenizer</strong> (5k vocab) achieving <strong>3.1
                                        char/token compression</strong> ratio</li>
                                <li>Curated corpus from <strong>Project Madurai</strong> and <strong>Tamil
                                        Wikipedia</strong>; deployed interactive demo on <strong><a
                                            href="https://huggingface.co/spaces/nirmalpratheep/TamilBPETokenizer"
                                            target="_blank">HuggingFace Spaces</a></strong></li>
                            </ul>
                        </div>
                        <div class="project">
                            <strong>vLLM Inference Benchmarking</strong>
                            <ul>
                                <li>Benchmarked <strong>vLLM inference</strong> on <strong>AMD GPU</strong> to measure
                                    <strong>TTFT</strong>, <strong>TPOT</strong>, <strong>ITL</strong>, and
                                    <strong>E2EL</strong> latency metrics
                                </li>
                                <li>Identified and analyzed <strong>performance bottlenecks</strong> in LLM serving
                                    infrastructure</li>
                            </ul>
                        </div>
                    </div>

                    <h3 class="category">Agentic AI Projects</h3>
                    <div class="project-list">
                        <div class="project">
                            <strong><a href="https://github.com/nirmalpratheep/codingAgent"
                                    target="_blank">codingAgent</a></strong>
                            <ul>
                                <li>Built <strong>8-Stage Bug Fixing Pipeline</strong> using <strong>Gemini 2.0
                                        Flash</strong> and <strong>AST-based Dependency Analysis</strong></li>
                                <li>Achieved <strong>90%+ success rate</strong> on tests by solving hidden dependencies
                                    and optimizing <strong>token budget</strong></li>
                            </ul>
                        </div>
                        <div class="project">
                            <strong><a href="https://github.com/nirmalpratheep/SWE-AgentBench"
                                    target="_blank">SWE-AgentBench</a></strong>
                            <ul>
                                <li>Developed <strong>multi-agent benchmark system</strong> with <strong>3-agent
                                        architecture</strong> (Evaluation Agent on Port 9001, Coding Agent on Port 9002,
                                    Reporting Agent on Port 9003) using agent-to-agent communication</li>
                                <li>Integrated <strong>Gemini 2.0 Flash LLM</strong> for code generation and evaluation
                                </li>
                                <li>Built <strong>Docker-based test isolation framework</strong> for secure and
                                    reproducible test execution on <strong>SWE-bench dataset</strong></li>
                                <li>Implemented <strong>automated repository setup</strong> with Git clone, checkout,
                                    and patch operations for test case preparation</li>
                                <li>Built <strong>test execution engine</strong> and <strong>reporting system</strong>
                                    for automated LLM performance benchmarking</li>
                            </ul>
                        </div>
                    </div>

                    <h3 class="category">ML Projects</h3>
                    <div class="project-list">
                        <div class="project">
                            <strong><a href="https://github.com/nirmalpratheep/ImageNetClassifier"
                                    target="_blank">ImageNetClassifier</a></strong>
                            <ul>
                                <li>Trained <strong>ResNet-50</strong> on <strong>ImageNet-1K (1.28M images)</strong>
                                    achieving <strong>77.4% Top-1 Accuracy</strong></li>
                                <li>Optimized training with <strong>CutMix</strong>, <strong>MixUp</strong>, and
                                    <strong>Random Erasing</strong> augmentations; used <strong>LR Finder</strong> for
                                    optimal convergence
                                </li>
                            </ul>
                        </div>
                        <div class="project">
                            <strong><a
                                    href="https://github.com/nirmalpratheep/MNISTImageClassifier-ArchitectureExploration"
                                    target="_blank">MNISTImageClassifier-ArchitectureExploration</a></strong>
                            <ul>
                                <li>Conducted <strong>CNN architecture search</strong> achieving <strong>99.50%
                                        accuracy</strong> with only <strong>17.3k parameters</strong></li>
                                <li>Analyzed <strong>Receptive Field vs Accuracy</strong> trade-offs; optimized depth
                                    and <strong>BatchNorm</strong> for efficiency</li>
                            </ul>
                        </div>
                        <div class="project">
                            <strong><a href="https://github.com/nirmalpratheep/CIFAR100-Resnet34"
                                    target="_blank">CIFAR100-Resnet34</a></strong>
                            <ul>
                                <li>Trained <strong>ResNet-34</strong> on <strong>CIFAR-100</strong>; achieved
                                    <strong>76.7% accuracy</strong>, <strong>77.1% precision</strong>
                                </li>
                                <li>Deployed interactive demo on <strong><a
                                            href="https://huggingface.co/spaces/nirmalpratheep/CIFAR100_ImageClassifier"
                                            target="_blank">HuggingFace Spaces</a></strong></li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section class="section two-col" id="skills">
                    <div>
                        <h2>üõ†Ô∏è Skills</h2>
                        <p><strong>ML/RL:</strong> PyTorch, HuggingFace, Stable Baselines 3, RL4CO, Gym, Ray</p>
                        <p><strong>Programming:</strong> Python, C++, Golang</p>
                        <p><strong>Infrastructure:</strong> Kubernetes, Docker, PostgreSQL, MongoDB</p>
                        <p><strong>AI/LLM:</strong> LLM Pre/PostTraining, Agentic Frameworks, GNN, Deep RL</p>
                    </div>
                    <div id="education">
                        <h2>üéì Education</h2>
                        <p><strong>M.Eng</strong> Electrical Engineering ‚Äî U. of Cincinnati, 2012</p>
                        <p><strong>B.Eng</strong> Electronics & Comm. ‚Äî Anna University, 2007</p>
                        <h2>üèÜ Honors</h2>
                        <p><strong>Top 15</strong> ‚Äî Innovate India Design Contest (ALTERA), 2007</p>
                        <p><strong>Elite Mentorship Program</strong> ‚Äî AMD</p>
                    </div>
                </section>

                <section class="section" id="certifications">
                    <h2>üìú Certifications</h2>
                    <p class="cert-line">
                        <span>Triton Kernel Development on AMD Instinct GPUs</span> ‚Ä¢
                        <span>LLM Serving Inference with VLLM and MI300X GPUs</span>
                        <span>Agentic Framework (HuggingFace)</span> ‚Ä¢
                        <span>Generative AI with LLMs (DeepLearning.AI)</span> ‚Ä¢
                        <span>ML OPS (DeepLearning.AI)</span> ‚Ä¢
                        <span>Machine Learning (Stanford)</span> ‚Ä¢
                        <span>Analytics Edge (MITx)</span> ‚Ä¢
                        <span>Parallel & Distributed Computing (Rice)</span> ‚Ä¢
                        <span>Kubernetes (Udacity)</span> ‚Ä¢
                        <span>BigData with Spark (Berkeley)</span> ‚Ä¢
                    </p>
                </section>
            </main>
        </div>
    </div>
    <footer class="footer">¬© 2024 Nirmal Pratheep Natarajan</footer>
</body>

</html>